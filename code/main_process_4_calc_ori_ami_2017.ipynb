{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62b13ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_process.py\n",
    "# 主程序入口，进行数据加载、预处理、模型训练与测试，并评估分类准确率\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from torch import nn\n",
    "from data_loader import *\n",
    "from sklearn.metrics import *\n",
    "from sklearn.preprocessing import *\n",
    "from collections import Counter\n",
    "from autoencoder import *\n",
    "from evt import *\n",
    "import argparse\n",
    "import time\n",
    "from datetime import timedelta\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "import warnings\n",
    "\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fb7b197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data size: 34096, After 10% sampling: 34096\n",
      "Class distribution after oversampling:\n",
      "0.1\n",
      "9     2347\n",
      "4     2322\n",
      "0     2322\n",
      "12    2300\n",
      "6     2288\n",
      "11    2287\n",
      "1     2286\n",
      "3     2283\n",
      "2     2273\n",
      "5     2261\n",
      "14    2258\n",
      "8     2247\n",
      "13    2233\n",
      "10    2210\n",
      "7     2179\n",
      "Name: count, dtype: int64\n",
      "=== Data Loading Time ===\n",
      "Data loading time: 0:00:37.100098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Study\\Github\\Trident\\code\\data_loader.py:47: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data = data.replace([np.inf, -np.inf], np.nan)\n"
     ]
    }
   ],
   "source": [
    "# 设置随机种子，保证实验可复现\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# 解析命令行参数，支持自定义数据集名称\n",
    "default_data_name = 'demo'\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--data_name', type = str, default = default_data_name, help = 'data name')\n",
    "# args = parser.parse_args()\n",
    "# data_name = args.data_name\n",
    "data_name = default_data_name\n",
    "\n",
    "# 加载数据集，并进行标准化\n",
    "# data_load: 特征数据, label_load: 标签\n",
    "data_load, label_load = read_dataset2017(data_name)\n",
    "data_load = StandardScaler().fit_transform(data_load)\n",
    "\n",
    "# 统计每个类别的样本数，确定训练/测试划分\n",
    "count_number = Counter(label_load)\n",
    "min_num = np.array(list(count_number.values())).min()  # 最小类别样本数\n",
    "test_per_class = 300  # 每类测试样本数\n",
    "num_per_class = min_num - test_per_class  # 每类训练样本数\n",
    "dim = data_load.shape[1]  # 特征维度\n",
    "b_size = test_per_class   # 批量大小\n",
    "loss_func = nn.MSELoss()  # 损失函数\n",
    "\n",
    "sum_num = len(set(list((label_load))))  # 类别总数\n",
    "train_num = 14  # 训练类别数（只用一个类别做已知，其余为新类）\n",
    "newclass_num = sum_num - train_num  # 新类别数\n",
    "\n",
    "# 打乱数据顺序\n",
    "shun = list(range(data_load.shape[0]))\n",
    "random.shuffle(shun)\n",
    "data_load = data_load[shun]\n",
    "label_load = label_load[shun]\n",
    "\n",
    "# 随机排列类别索引\n",
    "allIndex = np.random.permutation(train_num + newclass_num)\n",
    "# 计算数据加载部分耗费的时间\n",
    "data_loading_end = time.time()\n",
    "data_loading_time = data_loading_end - start_time\n",
    "print(f\"=== Data Loading Time ===\")\n",
    "print(f\"Data loading time: {str(timedelta(seconds=data_loading_time))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb67499a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct labels in 'gtlabel': [ 9. 11.  0. 13.  5.  8.  2.  1. 14.  4.  7. 10. 12.  3.  6.]\n",
      "\n",
      "=== Data Preprocessing Time ===\n",
      "Preprocessing time: 0:00:00.048612\n"
     ]
    }
   ],
   "source": [
    "# 构建训练集（只包含已知类别）\n",
    "data = np.zeros((num_per_class * (train_num), dim))\n",
    "label = np.zeros(num_per_class * (train_num))\n",
    "for pos in range(train_num):\n",
    "    i = allIndex[pos]\n",
    "    data[pos * num_per_class:(pos + 1) * num_per_class,:] = data_load[label_load==i][0:num_per_class, :]\n",
    "    label[pos * num_per_class:(pos + 1) * num_per_class] = i\n",
    "\n",
    "\n",
    "# 构建流式测试集（包含所有类别，已知类标记为原标签，未知类标记为999）\n",
    "streamdata = np.zeros((test_per_class * (train_num + newclass_num), dim))\n",
    "streamlabel = np.zeros(test_per_class * (train_num + newclass_num))\n",
    "gtlabel = np.zeros(test_per_class * (train_num + newclass_num))\n",
    "for pos in range(train_num + newclass_num):\n",
    "    i = allIndex[pos]\n",
    "    streamdata[pos * test_per_class:(pos + 1) * test_per_class,:] = data_load[label_load==i][-test_per_class:, :]\n",
    "    gtlabel[pos * test_per_class:(pos + 1) * test_per_class] = i\n",
    "    if pos < train_num:\n",
    "        streamlabel[pos * test_per_class:(pos+1) * test_per_class] = i\n",
    "    else:\n",
    "        streamlabel[pos * test_per_class:(pos + 1) * test_per_class] = 999\n",
    "        \n",
    "# 输出 gtlabel 中按出现顺序的不同标签\n",
    "unique_labels = pd.Series(gtlabel).unique()\n",
    "print(\"Distinct labels in 'gtlabel':\", unique_labels)\n",
    "\n",
    "\n",
    "# 根据标签统计，筛选样本数大于50的类别\n",
    "# 返回当前存在的类别列表\n",
    "def make_lab(label):\n",
    "    xianyou = pd.DataFrame(label).value_counts()\n",
    "    curr_lab = []\n",
    "    for j1 in xianyou.keys():\n",
    "        for i in range(train_num):\n",
    "            if (xianyou[j1] > 50) and (j1[0] == allIndex[i]):\n",
    "                curr_lab.append(j1[0])\n",
    "                break\n",
    "        # if xianyou[j1] > 50:\n",
    "        #     curr_lab.append(j1[0])\n",
    "    return curr_lab\n",
    "\n",
    "# 针对每个类别训练自编码器模型，并用SPOT方法确定阈值\n",
    "# 返回模型列表、阈值列表、类别列表\n",
    "def train(data, label, curr_lab):\n",
    "    mod_ls = []\n",
    "    thred_ls = []\n",
    "    class_ls = []\n",
    "    batch = 10\n",
    "    epoch = 10\n",
    "    y_in, y1, y2, y3, y4 = data_load.shape[1], 256, 128, 64, 32\n",
    "    for i in curr_lab:\n",
    "        class_ls.append(i)\n",
    "        model = Autoencoder(y_in, y1, y2, y3, y4)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr = 0.001, weight_decay = 5e-4)\n",
    "        # 训练自编码器\n",
    "        for i2 in range(epoch):\n",
    "            shun = list(range(data[label==i].shape[0]))\n",
    "            random.shuffle(shun)\n",
    "            for i3 in range(int(data[label==i].shape[0] / batch)):\n",
    "                data_input = torch.from_numpy(data[label==i][shun][i3 * batch : (i3+1) * batch]).float()\n",
    "                pred = model(data_input)\n",
    "                loss = loss_func(pred, data_input)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        mod_eva = model.eval()\n",
    "        mod_ls.append(model)\n",
    "        # 计算重构误差，用于阈值确定\n",
    "        mse_ls = []\n",
    "        for i4 in range(int(data[label==i].shape[0] / batch)):\n",
    "            data_input = torch.from_numpy(data[label==i][i4 * batch : (i4+1) * batch]).float()\n",
    "            pred = model(data_input)\n",
    "            for i5 in range(pred.shape[0]):\n",
    "                loss = loss_func(pred[i5], data_input[i5])\n",
    "                mse_ls.append(float(loss.detach().numpy()))\n",
    "        data_input = torch.from_numpy(data[label==i][(int(data[label==i].shape[0] / batch)) * batch:]).float()\n",
    "        pred = model(data_input)\n",
    "        for i5 in range(pred.shape[0]):\n",
    "            loss = loss_func(pred[i5], data_input[i5])\n",
    "            mse_ls.append(float(loss.detach().numpy()))\n",
    "        loss_list_use = np.array(mse_ls)\n",
    "        q = 5e-2 # 风险参数，可调\n",
    "        if len(loss_list_use) == 0:\n",
    "            thred_ls.append(0)\n",
    "            continue\n",
    "        try:\n",
    "            s = SPOT(q)\n",
    "            s.fit(loss_list_use, loss_list_use)\n",
    "            s.initialize()\n",
    "            results = s.run_simp()\n",
    "            # 阈值选取\n",
    "            if results['thresholds'][0] > 0:\n",
    "                thred_ls.append(results['thresholds'][0])\n",
    "            else:\n",
    "                thred_ls.append(np.sort(s.init_data)[int(0.85 * s.init_data.size)])\n",
    "        except Exception as e:\n",
    "            thred_ls.append(np.max(loss_list_use) + 1e-3)\n",
    "    return mod_ls, thred_ls, class_ls\n",
    "\n",
    "# 计算数据预处理部分耗费的时间\n",
    "preprocessing_end = time.time()\n",
    "preprocessing_time = preprocessing_end - data_loading_end\n",
    "\n",
    "print(f\"\\n=== Data Preprocessing Time ===\")\n",
    "print(f\"Preprocessing time: {str(timedelta(seconds=preprocessing_time))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f150f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Initializing ===\n",
      "\n",
      "=== Training Time ===\n",
      "Training time: 0:01:55.704786\n",
      "\n",
      "=== Known Class Predictions ===\n",
      "Number of samples: 4316\n",
      "Accuracy: 0.8047\n",
      "\n",
      "=== Ensemble Clustering on Unknown Features ===\n",
      "Clustering 184 unknown samples...\n",
      "Running K-means clustering...\n",
      "Running DBSCAN clustering...\n",
      "Running DEC clustering...\n",
      "Consensus matrix saved to ./output/Adjacency_Matrix.csv\n",
      "\n",
      "Finding optimal number of clusters...\n",
      "K=2, Silhouette Score=0.7571\n",
      "K=3, Silhouette Score=0.7671\n",
      "K=4, Silhouette Score=0.6837\n",
      "K=5, Silhouette Score=0.5285\n",
      "K=6, Silhouette Score=0.3915\n",
      "K=7, Silhouette Score=0.3869\n",
      "K=8, Silhouette Score=0.0622\n",
      "K=9, Silhouette Score=0.0372\n",
      "K=10, Silhouette Score=0.0372\n",
      "K=11, Silhouette Score=0.0269\n",
      "K=12, Silhouette Score=0.0378\n",
      "K=13, Silhouette Score=0.0378\n",
      "K=14, Silhouette Score=0.0515\n",
      "K=15, Silhouette Score=0.0515\n",
      "K=16, Silhouette Score=0.0458\n",
      "K=17, Silhouette Score=0.0458\n",
      "K=18, Silhouette Score=0.0520\n",
      "K=19, Silhouette Score=0.0520\n",
      "K=20, Silhouette Score=0.0520\n",
      "K=21, Silhouette Score=0.0692\n",
      "K=22, Silhouette Score=0.0692\n",
      "K=23, Silhouette Score=0.0692\n",
      "K=24, Silhouette Score=0.0692\n",
      "K=25, Silhouette Score=0.0692\n",
      "K=26, Silhouette Score=0.0692\n",
      "K=27, Silhouette Score=0.0597\n",
      "K=28, Silhouette Score=0.0597\n",
      "K=29, Silhouette Score=0.0597\n",
      "K=30, Silhouette Score=0.0597\n",
      "K=31, Silhouette Score=0.0597\n",
      "K=32, Silhouette Score=0.0597\n",
      "K=33, Silhouette Score=0.0597\n",
      "K=34, Silhouette Score=0.0597\n",
      "K=35, Silhouette Score=0.0597\n",
      "K=36, Silhouette Score=0.0408\n",
      "K=37, Silhouette Score=0.0408\n",
      "K=38, Silhouette Score=0.0408\n",
      "K=39, Silhouette Score=0.0408\n",
      "K=40, Silhouette Score=0.0408\n",
      "\n",
      "Optimal number of clusters: 3\n",
      "Note: Number of true clusters (1) differs from predicted clusters (3)\n",
      "\n",
      "=== Clustering Evaluation ===\n",
      "Number of samples clustered: 184\n",
      "True number of clusters: 1\n",
      "Predicted number of clusters: 3\n",
      "Adjusted Rand Index (ARI): 0.0000\n",
      "Adjusted Mutual Information (AMI): 0.0000\n",
      "\n",
      "Cluster distribution:\n",
      "Cluster 0: 101 samples\n",
      "Cluster 1: 56 samples\n",
      "Cluster 2: 27 samples\n",
      "\n",
      "=== Clustering Time ===\n",
      "Clustering time: 0:00:05.062295\n",
      "\n",
      "=== Total Time ===\n",
      "Total execution time: 0:02:37.915791\n",
      "\n",
      "=== Combined Evaluation (Known Classes + Clusters) ===\n",
      "Class ratio - Known:Unknown = 14:1\n",
      "Total samples evaluated: 4500\n",
      "Known class samples: 4316\n",
      "Clustered unknown samples: 184\n",
      "Combined Adjusted Mutual Information (AMI): 0.8027\n",
      "Combined Adjusted Rand Index (ARI): 0.7061\n"
     ]
    }
   ],
   "source": [
    "# 主流程入口\n",
    "if __name__ == '__main__':\n",
    "    print('=== Initializing ===')\n",
    "    # 获取当前类别\n",
    "    curr_lab = make_lab(label)\n",
    "    # 训练初始模型\n",
    "    mod_ls, thred_ls, class_ls = train(data, label, curr_lab)\n",
    "    \n",
    "    res_ls = []  # 预测结果列表\n",
    "    # 流式数据逐步推理与模型更新\n",
    "    for i5 in range(streamdata.shape[0]):\n",
    "        # 每处理完一个新类，更新模型\n",
    "        if i5 % b_size == 0 and int(i5 / b_size) > train_num:\n",
    "            updatedata = np.concatenate([data, streamdata[:i5]], axis=0)\n",
    "            updatelabel = np.concatenate([label, gtlabel[:i5]], axis=0)\n",
    "            curr_lab = make_lab(updatelabel)\n",
    "            print('Current labels:', curr_lab)\n",
    "            mod_ls, thred_ls, class_ls = train(updatedata, updatelabel, curr_lab) \n",
    "            print('*** Update model ***')\n",
    "            print(f'Number of models: {len(mod_ls)}')\n",
    "            print(f'Thresholds: {thred_ls}')\n",
    "            print(f'Class labels: {class_ls}')\n",
    "        # 对当前样本用所有模型计算重构误差\n",
    "        data_input = torch.from_numpy(streamdata[i5]).float()\n",
    "        mse_test = []\n",
    "        for model in mod_ls:\n",
    "            mod_eva = model.eval()\n",
    "            pred = model(data_input)\n",
    "            loss = loss_func(pred, data_input)\n",
    "            mse_test.append(float(loss.detach().numpy()))\n",
    "        # 判断是否为新类\n",
    "        cand_res = np.array(mse_test)[np.array(mse_test) < np.array(thred_ls)]\n",
    "        if len(cand_res) == 0:\n",
    "            res_ls.append(999)\n",
    "        else:\n",
    "            min_loss_res = cand_res.min()\n",
    "            res_ls.append(class_ls[mse_test.index(min_loss_res)])\n",
    "    \n",
    "    # Calculate training and testing time\n",
    "    training_end = time.time()\n",
    "    training_time = training_end - preprocessing_end\n",
    "\n",
    "    print(f\"\\n=== Training Time ===\")\n",
    "    print(f\"Training time: {str(timedelta(seconds=training_time))}\")\n",
    "\n",
    "    # print()\n",
    "    # print(\"Number of models:\", len(mod_ls))\n",
    "    # print(\"Thresholds:\", thred_ls)\n",
    "    # print(\"Class labels:\", class_ls)\n",
    "    # Output complete res_ls results\n",
    "    # print(\"=== Prediction Results ===\")\n",
    "    # print(\"res_ls contents:\", res_ls)\n",
    "\n",
    "    # Show data distribution in res_ls\n",
    "    # res_distribution = Counter(res_ls)\n",
    "    # print(\"=== Data Distribution ===\")\n",
    "    # for class_label, count in res_distribution.items():\n",
    "    #     print(f\"Class {class_label}: {count} samples\")\n",
    "\n",
    "    # 对新类样本，将999替换为真实标签\n",
    "    # for ii in range(train_num + newclass_num):\n",
    "    #     if ii >= train_num:\n",
    "    #         rep_npy = np.array(res_ls[test_per_class * ii : test_per_class * (ii + 1)])\n",
    "    #         rep_npy2 = rep_npy.copy()\n",
    "    #         rep_npy[rep_npy2==999] = allIndex[ii]\n",
    "    #         res_ls[test_per_class * ii:test_per_class * (ii + 1)] = list(rep_npy)\n",
    "    \n",
    "    # 计算准确率\n",
    "    # y_pred = np.array(res_ls)\n",
    "    # y_true = gtlabel[:len(res_ls)].copy()\n",
    "    # acc = accuracy_score(y_true, y_pred)\n",
    "    # print('Dataset:', data_name)\n",
    "    # print('Accuracy:', acc)\n",
    "\n",
    "    # Separate samples into known and unknown predictions\n",
    "    predict_known_features = []\n",
    "    predict_known_labels = []\n",
    "    predict_known_true_labels = []\n",
    "\n",
    "    predict_unknown_features = []\n",
    "    predict_unknown_labels = []\n",
    "    predict_unknown_true_labels = []\n",
    "    predict_known_indices = []\n",
    "\n",
    "    for i in range(test_per_class * train_num):\n",
    "        # Known class prediction\n",
    "        predict_known_features.append(streamdata[i])\n",
    "        predict_known_labels.append(res_ls[i])\n",
    "        predict_known_true_labels.append(gtlabel[i])\n",
    "        # Track the index in the original streamdata for known class predictions\n",
    "        predict_known_indices.append(i)\n",
    "\n",
    "\n",
    "    for i, pred_label in enumerate(res_ls):\n",
    "        if (i < test_per_class * train_num):\n",
    "            continue\n",
    "        if pred_label == 999:\n",
    "            # Unknown class prediction\n",
    "            predict_unknown_features.append(streamdata[i])\n",
    "            predict_unknown_labels.append(999)\n",
    "            predict_unknown_true_labels.append(gtlabel[i])\n",
    "        else:\n",
    "            # Known class prediction\n",
    "            predict_known_features.append(streamdata[i])\n",
    "            predict_known_labels.append(pred_label)\n",
    "            predict_known_true_labels.append(gtlabel[i])\n",
    "            # Track the index in the original streamdata for known class predictions\n",
    "            predict_known_indices.append(i)\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    predict_known_features = np.array(predict_known_features)\n",
    "    predict_known_labels = np.array(predict_known_labels)\n",
    "    predict_known_true_labels = np.array(predict_known_true_labels)\n",
    "\n",
    "    predict_unknown_features = np.array(predict_unknown_features)\n",
    "    predict_unknown_labels = np.array(predict_unknown_labels)\n",
    "    predict_unknown_true_labels = np.array(predict_unknown_true_labels)\n",
    "\n",
    "    \n",
    "    # Calculate accuracy for known predictions\n",
    "    if len(predict_known_labels) > 0:\n",
    "        known_accuracy = accuracy_score(predict_known_true_labels, predict_known_labels)\n",
    "        print(\"\\n=== Known Class Predictions ===\")\n",
    "        print(f\"Number of samples: {len(predict_known_labels)}\")\n",
    "        print(f\"Accuracy: {known_accuracy:.4f}\")\n",
    "    else:\n",
    "        print(\"\\nNo known class predictions to evaluate.\")\n",
    "    \n",
    "    # Ensemble Clustering\n",
    "    print(\"\\n=== Ensemble Clustering on Unknown Features ===\")\n",
    "    true_labels = []\n",
    "    predicted_clusters = []\n",
    "\n",
    "    # Skip clustering if there are no unknown predictions\n",
    "    if len(predict_unknown_features) == 0:\n",
    "        print(\"No unknown samples detected for clustering.\")\n",
    "    else:\n",
    "        print(f\"Clustering {len(predict_unknown_features)} unknown samples...\")\n",
    "        \n",
    "        # 1. Initialize consensus matrix\n",
    "        n_samples = len(predict_unknown_features)\n",
    "        consensus_matrix = np.zeros((n_samples, n_samples))\n",
    "        \n",
    "        # Normalize the data for better clustering results\n",
    "        scaler = StandardScaler()\n",
    "        normalized_features = scaler.fit_transform(predict_unknown_features)\n",
    "        \n",
    "        # 2. Perform multiple clustering with different algorithms and parameters\n",
    "        warnings.filterwarnings('ignore')\n",
    "        \n",
    "        # 2.1 K-means with different parameters (35 times)\n",
    "        print(\"Running K-means clustering...\")\n",
    "        for k in range(2, 12):  # Different number of clusters\n",
    "            for random_state in range(5):  # Different initializations\n",
    "                kmeans = KMeans(n_clusters=k, random_state=random_state)\n",
    "                labels = kmeans.fit_predict(normalized_features)\n",
    "                \n",
    "                # Update consensus matrix\n",
    "                for i in range(n_samples):\n",
    "                    for j in range(i+1, n_samples):\n",
    "                        if labels[i] == labels[j]:\n",
    "                            consensus_matrix[i, j] += 1\n",
    "                            consensus_matrix[j, i] += 1\n",
    "        \n",
    "        # 2.2 DBSCAN with different parameters (35 times)\n",
    "        print(\"Running DBSCAN clustering...\")\n",
    "        for eps in [0.1, 0.2, 0.3, 0.5, 0.7, 1.0, 1.2]:\n",
    "            for min_samples in [3, 5, 7, 10, 15]:\n",
    "                dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "                labels = dbscan.fit_predict(normalized_features)\n",
    "                \n",
    "                # Filter out noise points (label -1)\n",
    "                valid_indices = labels != -1\n",
    "                if sum(valid_indices) < 2:  # Skip if too many noise points\n",
    "                    continue\n",
    "                    \n",
    "                # Update consensus matrix for non-noise points\n",
    "                for i in range(n_samples):\n",
    "                    if labels[i] == -1:\n",
    "                        continue\n",
    "                    for j in range(i+1, n_samples):\n",
    "                        if labels[j] != -1 and labels[i] == labels[j]:\n",
    "                            consensus_matrix[i, j] += 1\n",
    "                            consensus_matrix[j, i] += 1\n",
    "        \n",
    "        # 2.3 Deep Embedded Clustering (DEC) with different parameters (30 times)\n",
    "        print(\"Running DEC clustering...\")\n",
    "        try:\n",
    "            import torch.nn as nn\n",
    "            \n",
    "            class DECAutoencoder(nn.Module):\n",
    "                def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "                    super(DECAutoencoder, self).__init__()\n",
    "                    self.encoder = nn.Sequential(\n",
    "                        nn.Linear(input_dim, hidden_dim),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(hidden_dim, latent_dim)\n",
    "                    )\n",
    "                    self.decoder = nn.Sequential(\n",
    "                        nn.Linear(latent_dim, hidden_dim),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(hidden_dim, input_dim)\n",
    "                    )\n",
    "                    \n",
    "                def forward(self, x):\n",
    "                    z = self.encoder(x)\n",
    "                    x_recon = self.decoder(z)\n",
    "                    return x_recon, z\n",
    "            \n",
    "            # Function to perform DEC clustering\n",
    "            def run_dec(features, n_clusters, hidden_dim, latent_dim, epochs=30):\n",
    "                input_dim = features.shape[1]\n",
    "                model = DECAutoencoder(input_dim, hidden_dim, latent_dim)\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "                \n",
    "                # Train autoencoder\n",
    "                X_tensor = torch.FloatTensor(features)\n",
    "                for epoch in range(epochs):\n",
    "                    model.train()\n",
    "                    x_recon, z = model(X_tensor)\n",
    "                    loss = nn.MSELoss()(x_recon, X_tensor)\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                # Extract latent features\n",
    "                model.eval()\n",
    "                _, z = model(X_tensor)\n",
    "                latent_features = z.detach().numpy()\n",
    "                \n",
    "                # Apply KMeans on latent features\n",
    "                kmeans = KMeans(n_clusters=n_clusters)\n",
    "                labels = kmeans.fit_predict(latent_features)\n",
    "                return labels\n",
    "            \n",
    "            # Run DEC with different parameters\n",
    "            for n_clusters in [3, 4, 5, 6, 7, 8]:\n",
    "                for hidden_dim in [64, 128]:\n",
    "                    for latent_dim in [32, 48, 64]:\n",
    "                        try:\n",
    "                            labels = run_dec(normalized_features, n_clusters, hidden_dim, latent_dim)\n",
    "                            \n",
    "                            # Update consensus matrix\n",
    "                            for i in range(n_samples):\n",
    "                                for j in range(i+1, n_samples):\n",
    "                                    if labels[i] == labels[j]:\n",
    "                                        consensus_matrix[i, j] += 1\n",
    "                                        consensus_matrix[j, i] += 1\n",
    "                        except Exception as e:\n",
    "                            print(f\"DEC failed with parameters: {n_clusters}, {hidden_dim}, {latent_dim}. Error: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping DEC due to error: {e}\")\n",
    "        \n",
    "        # 3. Normalize consensus matrix\n",
    "        # total_clusterings = 35 + 35 + 30  # total number of clustering runs\n",
    "        # consensus_matrix = consensus_matrix / total_clusterings\n",
    "        \n",
    "        # Save consensus matrix\n",
    "        os.makedirs('./output', exist_ok=True)\n",
    "        pd.DataFrame(consensus_matrix).to_csv('./output/Adjacency_Matrix.csv', index=False)\n",
    "        print(\"Consensus matrix saved to ./output/Adjacency_Matrix.csv\")\n",
    "        \n",
    "        # 4. Find optimal number of clusters using silhouette score\n",
    "        print(\"\\nFinding optimal number of clusters...\")\n",
    "        max_silhouette = -1\n",
    "        best_k = 2  # default\n",
    "        silhouette_scores = []\n",
    "        \n",
    "        for k in range(2, min(41, n_samples)):\n",
    "            kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "            labels = kmeans.fit_predict(consensus_matrix)\n",
    "            \n",
    "            try:\n",
    "                silhouette = silhouette_score(consensus_matrix, labels)\n",
    "                silhouette_scores.append(silhouette)\n",
    "                print(f\"K={k}, Silhouette Score={silhouette:.4f}\")\n",
    "                \n",
    "                if silhouette > max_silhouette:\n",
    "                    max_silhouette = silhouette\n",
    "                    best_k = k\n",
    "            except:\n",
    "                print(f\"Could not compute silhouette score for k={k}\")\n",
    "        \n",
    "        print(f\"\\nOptimal number of clusters: {best_k}\")\n",
    "        \n",
    "        # Perform final clustering with optimal k\n",
    "        final_kmeans = KMeans(n_clusters=best_k, random_state=42)\n",
    "        predicted_clusters = final_kmeans.fit_predict(consensus_matrix)\n",
    "        \n",
    "        # 5. Evaluate clustering performance\n",
    "        true_labels = predict_unknown_true_labels\n",
    "        \n",
    "        # If the number of unique true labels doesn't match predicted clusters,\n",
    "        # we need to find the best mapping\n",
    "        if len(np.unique(true_labels)) != len(np.unique(predicted_clusters)):\n",
    "            print(f\"Note: Number of true clusters ({len(np.unique(true_labels))}) differs from predicted clusters ({len(np.unique(predicted_clusters))})\")\n",
    "        \n",
    "        # Calculate metrics\n",
    "        ari = adjusted_rand_score(true_labels, predicted_clusters)\n",
    "        ami = adjusted_mutual_info_score(true_labels, predicted_clusters)\n",
    "        \n",
    "        print(\"\\n=== Clustering Evaluation ===\")\n",
    "        print(f\"Number of samples clustered: {len(true_labels)}\")\n",
    "        print(f\"True number of clusters: {len(np.unique(true_labels))}\")\n",
    "        print(f\"Predicted number of clusters: {len(np.unique(predicted_clusters))}\")\n",
    "        print(f\"Adjusted Rand Index (ARI): {ari:.4f}\")\n",
    "        print(f\"Adjusted Mutual Information (AMI): {ami:.4f}\")\n",
    "        \n",
    "        # Display cluster distribution\n",
    "        print(\"\\nCluster distribution:\")\n",
    "        cluster_counts = pd.Series(predicted_clusters).value_counts().sort_index()\n",
    "        for cluster, count in cluster_counts.items():\n",
    "            print(f\"Cluster {cluster}: {count} samples\")\n",
    "\n",
    "    # Calculate clustering time and total execution time\n",
    "    clustering_end = time.time()\n",
    "    clustering_time = clustering_end - training_end\n",
    "\n",
    "    print(f\"\\n=== Clustering Time ===\")\n",
    "    print(f\"Clustering time: {str(timedelta(seconds=clustering_time))}\")\n",
    "\n",
    "\n",
    "    print(f\"\\n=== Total Time ===\")\n",
    "    total_time = clustering_end - start_time\n",
    "    print(f\"Total execution time: {str(timedelta(seconds=total_time))}\")\n",
    "\n",
    "    # Combine the known class predictions and unknown class clusterings for evaluation\n",
    "\n",
    "    # 1. Create combined true labels array\n",
    "    # First, convert all arrays to numpy arrays if they aren't already\n",
    "    predict_known_true_labels_np = np.array(predict_known_true_labels)\n",
    "    true_labels_np = np.array(true_labels)\n",
    "    predict_known_labels_np = np.array(predict_known_labels)\n",
    "    predicted_clusters_np = np.array(predicted_clusters)\n",
    "\n",
    "    # 2. Offset the cluster labels to avoid overlap with known class labels\n",
    "    # Add an offset (e.g., 100) to the cluster labels\n",
    "    offset = 100\n",
    "    predicted_clusters_offset = predicted_clusters_np + offset\n",
    "\n",
    "    # 3. Create combined arrays for predictions and true labels\n",
    "    combined_predictions = np.concatenate([predict_known_labels_np, predicted_clusters_offset])\n",
    "    combined_true_labels = np.concatenate([predict_known_true_labels_np, true_labels_np])\n",
    "\n",
    "    # 4. Calculate metrics on the combined data\n",
    "    combined_ami = adjusted_mutual_info_score(combined_true_labels, combined_predictions)\n",
    "    combined_ari = adjusted_rand_score(combined_true_labels, combined_predictions)\n",
    "\n",
    "    # 5. Display results\n",
    "    print(\"\\n=== Combined Evaluation (Known Classes + Clusters) ===\")\n",
    "    print(f\"Class ratio - Known:Unknown = {train_num}:{newclass_num}\")\n",
    "    print(f\"Total samples evaluated: {len(combined_predictions)}\")\n",
    "    print(f\"Known class samples: {len(predict_known_labels_np)}\")\n",
    "    print(f\"Clustered unknown samples: {len(predicted_clusters_np)}\")\n",
    "    print(f\"Combined Adjusted Mutual Information (AMI): {combined_ami:.4f}\")\n",
    "    print(f\"Combined Adjusted Rand Index (ARI): {combined_ari:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fba01e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results have been saved to txt\n"
     ]
    }
   ],
   "source": [
    "with open(f'./output/整理/2017_{train_num}-{newclass_num}.txt', 'w') as f:\n",
    "    # Write header\n",
    "    f.write(f\"=== Experiment Results: Known Classes ({train_num}) vs Unknown Classes ({newclass_num}) ===\\n\\n\")\n",
    "    \n",
    "    # Write timing information\n",
    "    f.write(\"=== Timing Information ===\\n\")\n",
    "    f.write(f\"Data loading time: {data_loading_time:.4f} seconds\\n\")\n",
    "    f.write(f\"Preprocessing time: {preprocessing_time:.4f} seconds\\n\")\n",
    "    f.write(f\"Training time: {training_time:.4f} seconds\\n\")\n",
    "    f.write(f\"Clustering time: {clustering_time:.4f} seconds\\n\")\n",
    "    f.write(f\"Total execution time: {total_time:.4f} seconds\\n\\n\")\n",
    "    \n",
    "    # Write model information\n",
    "    f.write(\"=== Model Information ===\\n\")\n",
    "    f.write(f\"Number of models: {len(mod_ls)}\\n\")\n",
    "    f.write(f\"Class labels: {class_ls}\\n\")\n",
    "    f.write(f\"Thresholds: {thred_ls}\\n\\n\")\n",
    "    \n",
    "    # Write classification results\n",
    "    f.write(\"=== Classification Results ===\\n\")\n",
    "    f.write(f\"Known class accuracy: {known_accuracy:.4f}\\n\")\n",
    "    f.write(f\"Total samples: {len(combined_predictions)}\\n\")\n",
    "    f.write(f\"Known class samples: {len(predict_known_labels_np)}\\n\")\n",
    "    f.write(f\"Unknown class samples: {len(predict_unknown_features)}\\n\\n\")\n",
    "    \n",
    "    if (newclass_num != 0):\n",
    "        # Write clustering results\n",
    "        f.write(\"=== Clustering Results ===\\n\")\n",
    "        f.write(f\"Best number of clusters: {best_k}\\n\")\n",
    "        f.write(f\"True number of clusters: {len(np.unique(predict_unknown_true_labels))}\\n\")\n",
    "        f.write(f\"Adjusted Rand Index (ARI): {ari:.4f}\\n\")\n",
    "        f.write(f\"Adjusted Mutual Information (AMI): {ami:.4f}\\n\\n\")\n",
    "        \n",
    "    # Write combined evaluation\n",
    "    f.write(\"=== Combined Evaluation ===\\n\")\n",
    "    f.write(f\"Combined Adjusted Rand Index (ARI): {combined_ari:.4f}\\n\")\n",
    "    f.write(f\"Combined Adjusted Mutual Information (AMI): {combined_ami:.4f}\\n\\n\")\n",
    "    \n",
    "    if (newclass_num != 0):\n",
    "        # Write cluster distrib ution\n",
    "        f.write(\"=== Cluster Distribution ===\\n\")\n",
    "        for cluster, count in cluster_counts.items():\n",
    "            f.write(f\"Cluster {cluster}: {count} samples\\n\")\n",
    "\n",
    "print(f\"Results have been saved to txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trident_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
