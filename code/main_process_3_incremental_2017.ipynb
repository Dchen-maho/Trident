{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "594519b7",
   "metadata": {},
   "source": [
    "初始已知【2】类，增量每次增【2】类，直至囊括所有类别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62b13ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_process.py\n",
    "# 主程序入口，进行数据加载、预处理、模型训练与测试，并评估分类准确率\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from torch import nn\n",
    "from data_loader import *\n",
    "from sklearn.metrics import *\n",
    "from sklearn.preprocessing import *\n",
    "from collections import Counter\n",
    "from autoencoder import *\n",
    "from evt import *\n",
    "import argparse\n",
    "import time\n",
    "from datetime import timedelta\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "import warnings\n",
    "\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb7b197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data size: 68193, After 1% sampling: 68193\n",
      "Class distribution after oversampling:\n",
      "0.1\n",
      "9     4643\n",
      "4     4624\n",
      "1     4602\n",
      "0     4588\n",
      "6     4568\n",
      "2     4566\n",
      "14    4565\n",
      "3     4563\n",
      "8     4557\n",
      "12    4530\n",
      "5     4511\n",
      "11    4504\n",
      "7     4491\n",
      "10    4443\n",
      "13    4438\n",
      "Name: count, dtype: int64\n",
      "=== Data Loading Time ===\n",
      "Data loading time: 0:00:34.000428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Study\\Github\\Trident\\code\\data_loader.py:47: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data = data.replace([np.inf, -np.inf], np.nan)\n"
     ]
    }
   ],
   "source": [
    "# 设置随机种子，保证实验可复现\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# 解析命令行参数，支持自定义数据集名称\n",
    "default_data_name = 'demo'\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--data_name', type = str, default = default_data_name, help = 'data name')\n",
    "# args = parser.parse_args()\n",
    "# data_name = args.data_name\n",
    "data_name = default_data_name\n",
    "\n",
    "# 加载数据集，并进行标准化\n",
    "# data_load: 特征数据, label_load: 标签\n",
    "data_load, label_load = read_dataset2017(data_name)\n",
    "data_load = StandardScaler().fit_transform(data_load)\n",
    "\n",
    "# 统计每个类别的样本数，确定训练/测试划分\n",
    "count_number = Counter(label_load)\n",
    "min_num = np.array(list(count_number.values())).min()  # 最小类别样本数\n",
    "test_per_class = 300  # 每类测试样本数\n",
    "num_per_class = min_num - test_per_class  # 每类训练样本数\n",
    "dim = data_load.shape[1]  # 特征维度\n",
    "b_size = test_per_class   # 批量大小\n",
    "loss_func = nn.MSELoss()  # 损失函数\n",
    "\n",
    "sum_num = len(set(list((label_load))))  # 类别总数\n",
    "train_num = 2  # 训练类别数（只用一个类别做已知，其余为新类）\n",
    "newclass_num = sum_num - train_num  # 新类别数\n",
    "\n",
    "# 打乱数据顺序\n",
    "shun = list(range(data_load.shape[0]))\n",
    "random.shuffle(shun)\n",
    "data_load = data_load[shun]\n",
    "label_load = label_load[shun]\n",
    "\n",
    "# 按数字顺序排列类别索引，而不是随机排列\n",
    "# allIndex = np.arange(train_num + newclass_num)\n",
    "allIndex = np.random.permutation(train_num + newclass_num)\n",
    "\n",
    "\n",
    "# 计算数据加载部分耗费的时间\n",
    "data_loading_end = time.time()\n",
    "data_loading_time = data_loading_end - start_time\n",
    "print(f\"=== Data Loading Time ===\")\n",
    "print(f\"Data loading time: {str(timedelta(seconds=data_loading_time))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb67499a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct labels in 'gtlabel': [ 9. 11.  0. 13.  5.  8.  2.  1. 14.  4.  7. 10. 12.  3.  6.]\n",
      "\n",
      "=== Data Preprocessing Time ===\n",
      "Preprocessing time: 0:00:00.042140\n"
     ]
    }
   ],
   "source": [
    "# 构建训练集（只包含已知类别）\n",
    "data = np.zeros((num_per_class * (train_num), dim))\n",
    "label = np.zeros(num_per_class * (train_num))\n",
    "for pos in range(train_num):\n",
    "    i = allIndex[pos]\n",
    "    data[pos * num_per_class:(pos + 1) * num_per_class,:] = data_load[label_load==i][0:num_per_class, :]\n",
    "    label[pos * num_per_class:(pos + 1) * num_per_class] = i\n",
    "\n",
    "\n",
    "# 构建流式测试集（包含所有类别，已知类标记为原标签，未知类标记为999）\n",
    "streamdata = np.zeros((test_per_class * (train_num + newclass_num), dim))\n",
    "streamlabel = np.zeros(test_per_class * (train_num + newclass_num))\n",
    "gtlabel = np.zeros(test_per_class * (train_num + newclass_num))\n",
    "for pos in range(train_num + newclass_num):\n",
    "    i = allIndex[pos]\n",
    "    streamdata[pos * test_per_class:(pos + 1) * test_per_class,:] = data_load[label_load==i][-test_per_class:, :]\n",
    "    gtlabel[pos * test_per_class:(pos + 1) * test_per_class] = i\n",
    "    if pos < train_num:\n",
    "        streamlabel[pos * test_per_class:(pos+1) * test_per_class] = i\n",
    "    else:\n",
    "        streamlabel[pos * test_per_class:(pos + 1) * test_per_class] = 999\n",
    "        \n",
    "# 输出 gtlabel 中按出现顺序的不同标签\n",
    "unique_labels = pd.Series(gtlabel).unique()\n",
    "print(\"Distinct labels in 'gtlabel':\", unique_labels)\n",
    "\n",
    "\n",
    "# 根据标签统计，筛选样本数大于50的类别\n",
    "# 返回当前存在的类别列表\n",
    "def make_lab(label):\n",
    "    xianyou = pd.DataFrame(label).value_counts()\n",
    "    curr_lab = []\n",
    "    for j1 in xianyou.keys():\n",
    "        for i in range(train_num):\n",
    "            if (xianyou[j1] > 50) and (j1[0] == allIndex[i]):\n",
    "                curr_lab.append(j1[0])\n",
    "                break\n",
    "        # if xianyou[j1] > 50:\n",
    "        #     curr_lab.append(j1[0])\n",
    "    return curr_lab\n",
    "\n",
    "# 针对每个类别训练自编码器模型，并用SPOT方法确定阈值\n",
    "# 返回模型列表、阈值列表、类别列表\n",
    "def train(data, label, curr_lab):\n",
    "    mod_ls = []\n",
    "    thred_ls = []\n",
    "    class_ls = []\n",
    "    batch = 10\n",
    "    epoch = 10\n",
    "    y_in, y1, y2, y3, y4 = data_load.shape[1], 256, 128, 64, 32\n",
    "    for i in curr_lab:\n",
    "        class_ls.append(i)\n",
    "        model = Autoencoder(y_in, y1, y2, y3, y4)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr = 0.001, weight_decay = 5e-4)\n",
    "        # 训练自编码器\n",
    "        for i2 in range(epoch):\n",
    "            shun = list(range(data[label==i].shape[0]))\n",
    "            random.shuffle(shun)\n",
    "            for i3 in range(int(data[label==i].shape[0] / batch)):\n",
    "                data_input = torch.from_numpy(data[label==i][shun][i3 * batch : (i3+1) * batch]).float()\n",
    "                pred = model(data_input)\n",
    "                loss = loss_func(pred, data_input)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        mod_eva = model.eval()\n",
    "        mod_ls.append(model)\n",
    "        # 计算重构误差，用于阈值确定\n",
    "        mse_ls = []\n",
    "        for i4 in range(int(data[label==i].shape[0] / batch)):\n",
    "            data_input = torch.from_numpy(data[label==i][i4 * batch : (i4+1) * batch]).float()\n",
    "            pred = model(data_input)\n",
    "            for i5 in range(pred.shape[0]):\n",
    "                loss = loss_func(pred[i5], data_input[i5])\n",
    "                mse_ls.append(float(loss.detach().numpy()))\n",
    "        data_input = torch.from_numpy(data[label==i][(int(data[label==i].shape[0] / batch)) * batch:]).float()\n",
    "        pred = model(data_input)\n",
    "        for i5 in range(pred.shape[0]):\n",
    "            loss = loss_func(pred[i5], data_input[i5])\n",
    "            mse_ls.append(float(loss.detach().numpy()))\n",
    "        loss_list_use = np.array(mse_ls)\n",
    "        q = 5e-2 # 风险参数，可调\n",
    "        if len(loss_list_use) == 0:\n",
    "            thred_ls.append(0)\n",
    "            continue\n",
    "        # if len(loss_list_use) < 50 or np.all(loss_list_use == loss_list_use[0]):\n",
    "        #     thred_ls.append(np.max(loss_list_use) + 1e-3)\n",
    "        #     continue\n",
    "        try:\n",
    "            s = SPOT(q)\n",
    "            s.fit(loss_list_use, loss_list_use)\n",
    "            s.initialize()\n",
    "            results = s.run_simp()\n",
    "            # 阈值选取\n",
    "            if results['thresholds'][0] > 0:\n",
    "                thred_ls.append(results['thresholds'][0])\n",
    "            else:\n",
    "                thred_ls.append(np.sort(s.init_data)[int(0.85 * s.init_data.size)])\n",
    "        except Exception as e:\n",
    "            thred_ls.append(np.max(loss_list_use) + 1e-3)\n",
    "    return mod_ls, thred_ls, class_ls\n",
    "\n",
    "# 计算数据预处理部分耗费的时间\n",
    "preprocessing_end = time.time()\n",
    "preprocessing_time = preprocessing_end - data_loading_end\n",
    "\n",
    "print(f\"\\n=== Data Preprocessing Time ===\")\n",
    "print(f\"Preprocessing time: {str(timedelta(seconds=preprocessing_time))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b13dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_clustering(features, k=0, verbose=True):\n",
    "    if features.shape[0] == 0:\n",
    "        return np.array([]), np.array([])\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Clustering {len(features)} samples...\")\n",
    "    \n",
    "    # 1. Initialize consensus matrix\n",
    "    n_samples = len(features)\n",
    "    consensus_matrix = np.zeros((n_samples, n_samples))\n",
    "    \n",
    "    # Normalize the data for better clustering results\n",
    "    scaler = StandardScaler()\n",
    "    normalized_features = scaler.fit_transform(features)\n",
    "    \n",
    "    # 2. Perform multiple clustering with different algorithms and parameters\n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    # 2.1 K-means with different parameters\n",
    "    if verbose:\n",
    "        print(\"Running K-means clustering...\")\n",
    "    for cluster_k in range(2, 12):  # Different number of clusters\n",
    "        if n_samples <= cluster_k:\n",
    "            if verbose:\n",
    "                print(f\"Skipping k={cluster_k} since sample size {n_samples} is too small\")\n",
    "            break\n",
    "        for random_state in range(5):  # Different initializations\n",
    "            kmeans = KMeans(n_clusters=cluster_k, random_state=random_state)\n",
    "            labels = kmeans.fit_predict(normalized_features)\n",
    "            \n",
    "            # Update consensus matrix\n",
    "            for i in range(n_samples):\n",
    "                for j in range(i+1, n_samples):\n",
    "                    if labels[i] == labels[j]:\n",
    "                        consensus_matrix[i, j] += 1\n",
    "                        consensus_matrix[j, i] += 1\n",
    "    \n",
    "    # 2.2 DBSCAN with different parameters\n",
    "    if verbose:\n",
    "        print(\"Running DBSCAN clustering...\")\n",
    "    for eps in [0.1, 0.2, 0.3, 0.5, 0.7, 1.0, 1.2]:\n",
    "        for min_samples in [3, 5, 7, 10, 15]:\n",
    "            dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "            labels = dbscan.fit_predict(normalized_features)\n",
    "            \n",
    "            # Filter out noise points (label -1)\n",
    "            valid_indices = labels != -1\n",
    "            if sum(valid_indices) < 2:  # Skip if too many noise points\n",
    "                continue\n",
    "                \n",
    "            # Update consensus matrix for non-noise points\n",
    "            for i in range(n_samples):\n",
    "                if labels[i] == -1:\n",
    "                    continue\n",
    "                for j in range(i+1, n_samples):\n",
    "                    if labels[j] != -1 and labels[i] == labels[j]:\n",
    "                        consensus_matrix[i, j] += 1\n",
    "                        consensus_matrix[j, i] += 1\n",
    "    \n",
    "    # 2.3 Deep Embedded Clustering (DEC) with different parameters\n",
    "    if verbose:\n",
    "        print(\"Running DEC clustering...\")\n",
    "    try:\n",
    "        # Define DEC autoencoder class\n",
    "        class DECAutoencoder(nn.Module):\n",
    "            def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "                super(DECAutoencoder, self).__init__()\n",
    "                self.encoder = nn.Sequential(\n",
    "                    nn.Linear(input_dim, hidden_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(hidden_dim, latent_dim)\n",
    "                )\n",
    "                self.decoder = nn.Sequential(\n",
    "                    nn.Linear(latent_dim, hidden_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(hidden_dim, input_dim)\n",
    "                )\n",
    "                \n",
    "            def forward(self, x):\n",
    "                z = self.encoder(x)\n",
    "                x_recon = self.decoder(z)\n",
    "                return x_recon, z\n",
    "        \n",
    "        # Function to perform DEC clustering\n",
    "        def run_dec(features, n_clusters, hidden_dim, latent_dim, epochs=30):\n",
    "            input_dim = features.shape[1]\n",
    "            model = DECAutoencoder(input_dim, hidden_dim, latent_dim)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "            \n",
    "            # Train autoencoder\n",
    "            X_tensor = torch.FloatTensor(features)\n",
    "            for epoch in range(epochs):\n",
    "                model.train()\n",
    "                x_recon, z = model(X_tensor)\n",
    "                loss = nn.MSELoss()(x_recon, X_tensor)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            # Extract latent features\n",
    "            model.eval()\n",
    "            _, z = model(X_tensor)\n",
    "            latent_features = z.detach().numpy()\n",
    "            \n",
    "            # Apply KMeans on latent features\n",
    "            kmeans = KMeans(n_clusters=n_clusters)\n",
    "            labels = kmeans.fit_predict(latent_features)\n",
    "            return labels\n",
    "        \n",
    "        # Run DEC with different parameters\n",
    "        for n_clusters in [3, 4, 5, 6, 7, 8]:\n",
    "            for hidden_dim in [64, 128]:\n",
    "                for latent_dim in [32, 48, 64]:\n",
    "                    try:\n",
    "                        labels = run_dec(normalized_features, n_clusters, hidden_dim, latent_dim)\n",
    "                        \n",
    "                        # Update consensus matrix\n",
    "                        for i in range(n_samples):\n",
    "                            for j in range(i+1, n_samples):\n",
    "                                if labels[i] == labels[j]:\n",
    "                                    consensus_matrix[i, j] += 1\n",
    "                                    consensus_matrix[j, i] += 1\n",
    "                    except Exception as e:\n",
    "                        if verbose:\n",
    "                            print(f\"DEC failed with parameters: {n_clusters}, {hidden_dim}, {latent_dim}. Error: {e}\")\n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"Skipping DEC due to error: {e}\")\n",
    "    \n",
    "    # 3. If k is not specified, find optimal number of clusters using silhouette score\n",
    "    if k <= 0:\n",
    "        if verbose:\n",
    "            print(\"\\nFinding optimal number of clusters...\")\n",
    "        max_silhouette = -1\n",
    "        best_k = 2  # default\n",
    "        silhouette_scores = []\n",
    "        \n",
    "        for test_k in range(2, min(41, n_samples)):\n",
    "            kmeans = KMeans(n_clusters=test_k, random_state=42)\n",
    "            labels = kmeans.fit_predict(consensus_matrix)\n",
    "            \n",
    "            try:\n",
    "                silhouette = silhouette_score(consensus_matrix, labels)\n",
    "                silhouette_scores.append(silhouette)\n",
    "                if verbose:\n",
    "                    print(f\"K={test_k}, Silhouette Score={silhouette:.4f}\")\n",
    "                \n",
    "                if silhouette > max_silhouette:\n",
    "                    max_silhouette = silhouette\n",
    "                    best_k = test_k\n",
    "            except Exception as e:\n",
    "                if verbose:\n",
    "                    print(f\"Could not compute silhouette score for k={test_k}\")\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nOptimal number of clusters: {best_k}\")\n",
    "        final_k = best_k\n",
    "    else:\n",
    "        final_k = k\n",
    "        if verbose:\n",
    "            print(f\"\\nUsing specified number of clusters: {final_k}\")\n",
    "    \n",
    "    # 4. Perform final clustering with optimal/specified k\n",
    "    final_kmeans = KMeans(n_clusters=final_k, random_state=42)\n",
    "    predicted_clusters = final_kmeans.fit_predict(consensus_matrix)\n",
    "    \n",
    "    return predicted_clusters, consensus_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f150f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Initializing ===\n",
      "=== Incremental Progress: 4/15 classes ===\n",
      "Current labels: [ 9 11  0 13]\n",
      "Found 532 unknown samples for clustering\n",
      "Starting ensemble clustering to identify new classes...\n",
      "Ensemble Clustering AMI: 0.2001\n",
      "*** Update model with new classes ***\n",
      "Current Prediction Metrics:\n",
      "  Accuracy: 0.7967\n",
      "  F1 Score (macro): 0.6464\n",
      "  Recall (macro): 0.6373\n",
      "=== Incremental Progress: 6/15 classes ===\n",
      "Current labels: [ 9 11  0 13  5  8]\n",
      "Found 592 unknown samples for clustering\n",
      "Starting ensemble clustering to identify new classes...\n",
      "Ensemble Clustering AMI: 1.0000\n",
      "*** Update model with new classes ***\n",
      "Current Prediction Metrics:\n",
      "  Accuracy: 0.8600\n",
      "  F1 Score (macro): 0.7441\n",
      "  Recall (macro): 0.7371\n",
      "=== Incremental Progress: 8/15 classes ===\n",
      "Current labels: [ 9 11  0 13  5  8  2  1]\n",
      "Found 244 unknown samples for clustering\n",
      "Starting ensemble clustering to identify new classes...\n",
      "Ensemble Clustering AMI: 0.1802\n",
      "*** Update model with new classes ***\n",
      "Current Prediction Metrics:\n",
      "  Accuracy: 0.7129\n",
      "  F1 Score (macro): 0.6231\n",
      "  Recall (macro): 0.6337\n",
      "=== Incremental Progress: 10/15 classes ===\n",
      "Current labels: [ 9 11  0 13  5  8  2  1 14  4]\n",
      "Found 137 unknown samples for clustering\n",
      "Starting ensemble clustering to identify new classes...\n",
      "Ensemble Clustering AMI: 0.0000\n",
      "*** Update model with new classes ***\n",
      "Current Prediction Metrics:\n",
      "  Accuracy: 0.6020\n",
      "  F1 Score (macro): 0.5351\n",
      "  Recall (macro): 0.5473\n",
      "=== Incremental Progress: 12/15 classes ===\n",
      "Current labels: [ 9 11  0 13  5  8  2  1 14  4  7 10]\n",
      "Found 7 unknown samples for clustering\n",
      "Starting ensemble clustering to identify new classes...\n",
      "Ensemble Clustering AMI: -0.1122\n",
      "*** Update model with new classes ***\n",
      "Current Prediction Metrics:\n",
      "  Accuracy: 0.5025\n",
      "  F1 Score (macro): 0.4447\n",
      "  Recall (macro): 0.4638\n",
      "=== Incremental Progress: 14/15 classes ===\n",
      "Current labels: [ 9 11  0 13  5  8  2  1 14  4  7 10 12  3]\n",
      "Found 18 unknown samples for clustering\n",
      "Starting ensemble clustering to identify new classes...\n",
      "Ensemble Clustering AMI: 0.0000\n",
      "*** Update model with new classes ***\n",
      "Current Prediction Metrics:\n",
      "  Accuracy: 0.4326\n",
      "  F1 Score (macro): 0.3715\n",
      "  Recall (macro): 0.4038\n",
      "=== Incremental Progress: 15/15 classes ===\n",
      "Current labels: [ 9 11  0 13  5  8  2  1 14  4  7 10 12  3  6]\n",
      "=== Incremental Process Completed ===\n",
      "\n",
      "=== Final Classification Results ===\n",
      "Overall Accuracy: 0.4038\n",
      "F1 Score (macro): 0.3373\n",
      "Recall (macro): 0.3785\n",
      "Precision (macro): 0.3614\n",
      "\n",
      "=== Per-Class Metrics ===\n",
      "Class 0.0:\n",
      "  Precision: 0.1213\n",
      "  Recall: 0.7533\n",
      "  F1-score: 0.2090\n",
      "  Support: 300.0\n",
      "Class 1.0:\n",
      "  Precision: 0.1232\n",
      "  Recall: 0.0833\n",
      "  F1-score: 0.0994\n",
      "  Support: 300.0\n",
      "Class 2.0:\n",
      "  Precision: 1.0000\n",
      "  Recall: 0.4600\n",
      "  F1-score: 0.6301\n",
      "  Support: 300.0\n",
      "Class 3.0:\n",
      "  Precision: 0.1379\n",
      "  Recall: 0.0267\n",
      "  F1-score: 0.0447\n",
      "  Support: 300.0\n",
      "Class 4.0:\n",
      "  Precision: 0.7090\n",
      "  Recall: 0.3167\n",
      "  F1-score: 0.4378\n",
      "  Support: 300.0\n",
      "Class 5.0:\n",
      "  Precision: 0.7246\n",
      "  Recall: 0.9733\n",
      "  F1-score: 0.8307\n",
      "  Support: 300.0\n",
      "Class 6.0:\n",
      "  Precision: 0.0000\n",
      "  Recall: 0.0000\n",
      "  F1-score: 0.0000\n",
      "  Support: 300.0\n",
      "Class 7.0:\n",
      "  Precision: 0.0333\n",
      "  Recall: 0.0067\n",
      "  F1-score: 0.0111\n",
      "  Support: 300.0\n",
      "Class 8.0:\n",
      "  Precision: 1.0000\n",
      "  Recall: 1.0000\n",
      "  F1-score: 1.0000\n",
      "  Support: 300.0\n",
      "Class 9.0:\n",
      "  Precision: 0.7595\n",
      "  Recall: 1.0000\n",
      "  F1-score: 0.8633\n",
      "  Support: 300.0\n",
      "Class 10.0:\n",
      "  Precision: 0.0244\n",
      "  Recall: 0.0033\n",
      "  F1-score: 0.0059\n",
      "  Support: 300.0\n",
      "Class 11.0:\n",
      "  Precision: 0.7802\n",
      "  Recall: 0.8400\n",
      "  F1-score: 0.8090\n",
      "  Support: 300.0\n",
      "Class 12.0:\n",
      "  Precision: 0.0000\n",
      "  Recall: 0.0000\n",
      "  F1-score: 0.0000\n",
      "  Support: 300.0\n",
      "Class 13.0:\n",
      "  Precision: 0.3693\n",
      "  Recall: 0.5933\n",
      "  F1-score: 0.4552\n",
      "  Support: 300.0\n",
      "Class 14.0:\n",
      "  Precision: 0.0000\n",
      "  Recall: 0.0000\n",
      "  F1-score: 0.0000\n",
      "  Support: 300.0\n",
      "\n",
      "=== Clustering Time ===\n",
      "Clustering time: 0:05:49.743831\n",
      "\n",
      "=== Total Time ===\n",
      "Total execution time: 0:06:23.786398\n"
     ]
    }
   ],
   "source": [
    "# 主流程入口\n",
    "if __name__ == '__main__':\n",
    "    print('=== Initializing ===')\n",
    "    # 获取当前类别\n",
    "    curr_num = train_num\n",
    "    curr_lab = allIndex[:curr_num]\n",
    "    # 训练初始模型\n",
    "    mod_ls, thred_ls, class_ls = train(data, label, curr_lab)\n",
    "    \n",
    "    res_ls = []  # 预测结果列表\n",
    "    # 先预测已知的train_num个类别\n",
    "    for i5 in range(test_per_class * train_num):\n",
    "        # 对当前样本用所有模型计算重构误差\n",
    "        data_input = torch.from_numpy(streamdata[i5]).float()\n",
    "        mse_test = []\n",
    "        for model in mod_ls:\n",
    "            mod_eva = model.eval()\n",
    "            pred = model(data_input)\n",
    "            loss = loss_func(pred, data_input)\n",
    "            mse_test.append(float(loss.detach().numpy()))\n",
    "        # 判断是否为新类\n",
    "        cand_res = np.array(mse_test)[np.array(mse_test) < np.array(thred_ls)]\n",
    "        if len(cand_res) == 0:\n",
    "            res_ls.append(999)\n",
    "        else:\n",
    "            min_loss_res = cand_res.min()\n",
    "            res_ls.append(class_ls[mse_test.index(min_loss_res)])\n",
    "    updatedata = data\n",
    "    updatelabel = label\n",
    "    # 开始处理增量数据\n",
    "    for i in range(int((sum_num - train_num + 1) // 2)):\n",
    "        curr_num += 2\n",
    "        if curr_num <= sum_num:\n",
    "            print(f\"=== Incremental Progress: {curr_num}/{sum_num} classes ===\")\n",
    "            # 正常增量2类\n",
    "            curr_lab = allIndex[:curr_num]\n",
    "            print('Current labels:', curr_lab)\n",
    "            unknown_samples = []\n",
    "            unknown_indices = []\n",
    "            real_label = []\n",
    "            for i5 in range(test_per_class * (curr_num - 2), test_per_class * curr_num):\n",
    "                # 对当前样本用所有模型计算重构误差\n",
    "                data_input = torch.from_numpy(streamdata[i5]).float()\n",
    "                mse_test = []\n",
    "                for model in mod_ls:\n",
    "                    mod_eva = model.eval()\n",
    "                    pred = model(data_input)\n",
    "                    loss = loss_func(pred, data_input)\n",
    "                    mse_test.append(float(loss.detach().numpy()))\n",
    "                # 判断是否为新类\n",
    "                cand_res = np.array(mse_test)[np.array(mse_test) < np.array(thred_ls)]\n",
    "                if len(cand_res) == 0:\n",
    "                    res_ls.append(999)\n",
    "                    # Record the unknown sample and its index for later clustering\n",
    "                    unknown_samples.append(streamdata[i5])\n",
    "                    unknown_indices.append(i5)  # Index of the newly added 999 in res_ls\n",
    "                    # The streamdata index is i5, so we can get the true label from gtlabel\n",
    "                    real_label.append(gtlabel[i5])  # Get the true label for this unknown sample\n",
    "                else:\n",
    "                    min_loss_res = cand_res.min()\n",
    "                    res_ls.append(class_ls[mse_test.index(min_loss_res)])\n",
    "            # Check if we have enough unknown samples to cluster\n",
    "            if len(unknown_samples) > 1:\n",
    "                print(f\"Found {len(unknown_samples)} unknown samples for clustering\")\n",
    "                print(\"Starting ensemble clustering to identify new classes...\")\n",
    "                \n",
    "                # Convert the list of unknown samples to a numpy array\n",
    "                unknown_samples_array = np.array(unknown_samples)\n",
    "                \n",
    "                # Apply ensemble clustering to identify the two new classes\n",
    "                cluster_labels, _ = ensemble_clustering(unknown_samples_array, k=2, verbose=False)\n",
    "                \n",
    "                # Update the results list with the newly assigned cluster IDs\n",
    "                # Map cluster 0,1 to the next class IDs (which should be curr_num-2 and curr_num-1)\n",
    "                # Check which real labels are in each cluster and map accordingly\n",
    "                cluster_0_samples = [gtlabel[unknown_indices[i]] for i in range(len(unknown_indices)) if cluster_labels[i] == 0]\n",
    "\n",
    "                # Count occurrences of each label in cluster 0\n",
    "                cluster_0_counts = {}\n",
    "                for label in cluster_0_samples:\n",
    "                    cluster_0_counts[label] = cluster_0_counts.get(label, 0) + 1\n",
    "\n",
    "                # Determine which label is more common in cluster 0\n",
    "                if len(cluster_0_counts) > 0:\n",
    "                    most_common_label_in_cluster_0 = max(cluster_0_counts, key=cluster_0_counts.get)\n",
    "                    \n",
    "                    # If the most common label in cluster 0 is the first new class (curr_lab[-2]),\n",
    "                    # maintain original mapping, otherwise swap\n",
    "                    if most_common_label_in_cluster_0 == curr_lab[-2]:\n",
    "                        new_class_ids = [curr_lab[-2], curr_lab[-1]]\n",
    "                    else:\n",
    "                        new_class_ids = [curr_lab[-1], curr_lab[-2]]\n",
    "                else:\n",
    "                    new_class_ids = [curr_lab[-2], curr_lab[-1]]\n",
    "                \n",
    "                for i, idx in enumerate(unknown_indices):\n",
    "                    # Map the cluster label to the appropriate new class ID\n",
    "                    new_class_id = new_class_ids[cluster_labels[i]]\n",
    "                    res_ls[idx] = new_class_id\n",
    "                \n",
    "                # Calculate AMI for the ensemble clustering\n",
    "                ensemble_ami = adjusted_mutual_info_score(real_label, cluster_labels)\n",
    "                print(f\"Ensemble Clustering AMI: {ensemble_ami:.4f}\")\n",
    "\n",
    "                \n",
    "                # Train new models for these newly discovered classes\n",
    "                # Collect the newly discovered class samples for training\n",
    "                # Only add the samples we just clustered and labeled\n",
    "                new_class_data = []\n",
    "                new_class_labels = []\n",
    "                \n",
    "                for i, idx in enumerate(unknown_indices):\n",
    "                    new_class_data.append(streamdata[idx])\n",
    "                    new_class_labels.append(new_class_ids[cluster_labels[i]])\n",
    "                \n",
    "                # Convert to numpy arrays for concatenation\n",
    "                new_class_data = np.array(new_class_data)\n",
    "                new_class_labels = np.array(new_class_labels)\n",
    "                \n",
    "                # Update the training data with new class samples\n",
    "                updatedata = np.concatenate([updatedata, new_class_data], axis=0)\n",
    "                updatelabel = np.concatenate([updatelabel, new_class_labels], axis=0)\n",
    "                mod_ls, thred_ls, class_ls = train(updatedata, updatelabel, curr_lab)\n",
    "                print('*** Update model with new classes ***')\n",
    "            else:\n",
    "                print(\"No unknown samples found for clustering\")\n",
    "            # Calculate current prediction accuracy\n",
    "            # Get current predictions and true labels up to this point\n",
    "            current_preds = np.array(res_ls)\n",
    "            current_true = gtlabel[:len(res_ls)]\n",
    "            current_accuracy = accuracy_score(current_true, current_preds)\n",
    "\n",
    "            # Calculate F1 score and recall - using macro averaging to handle multiple classes\n",
    "            current_f1 = f1_score(current_true, current_preds, average='macro', zero_division=0)\n",
    "            current_recall = recall_score(current_true, current_preds, average='macro', zero_division=0)\n",
    "\n",
    "            print(f\"Current Prediction Metrics:\")\n",
    "            print(f\"  Accuracy: {current_accuracy:.4f}\")\n",
    "            print(f\"  F1 Score (macro): {current_f1:.4f}\")\n",
    "            print(f\"  Recall (macro): {current_recall:.4f}\")\n",
    "        else:\n",
    "            # 最后一次增量，可能只增加1类\n",
    "            curr_num = sum_num\n",
    "            print(f\"=== Incremental Progress: {curr_num}/{sum_num} classes ===\")\n",
    "            curr_lab = allIndex[:curr_num]\n",
    "            print('Current labels:', curr_lab)\n",
    "            for i5 in range(test_per_class * (curr_num - 1), test_per_class * curr_num):\n",
    "                # 对当前样本用所有模型计算重构误差\n",
    "                data_input = torch.from_numpy(streamdata[i5]).float()\n",
    "                mse_test = []\n",
    "                for model in mod_ls:\n",
    "                    mod_eva = model.eval()\n",
    "                    pred = model(data_input)\n",
    "                    loss = loss_func(pred, data_input)\n",
    "                    mse_test.append(float(loss.detach().numpy()))\n",
    "                # 判断是否为新类\n",
    "                cand_res = np.array(mse_test)[np.array(mse_test) < np.array(thred_ls)]\n",
    "                if len(cand_res) == 0:\n",
    "                    res_ls.append(curr_lab[-1])  # 最后一类\n",
    "                else:\n",
    "                    min_loss_res = cand_res.min()\n",
    "                    res_ls.append(class_ls[mse_test.index(min_loss_res)])\n",
    "        \n",
    "    print('=== Incremental Process Completed ===')\n",
    "    # 计算最终分类性能指标\n",
    "    print('\\n=== Final Classification Results ===')\n",
    "    # 将所有预测结果和真实标签转换为NumPy数组\n",
    "    all_predictions = np.array(res_ls)\n",
    "    all_true_labels = gtlabel[:len(res_ls)]\n",
    "\n",
    "    # 计算整体准确率\n",
    "    accuracy = accuracy_score(all_true_labels, all_predictions)\n",
    "    print(f'Overall Accuracy: {accuracy:.4f}')\n",
    "\n",
    "    # 计算F1分数 (宏平均)\n",
    "    f1 = f1_score(all_true_labels, all_predictions, average='macro', zero_division=0)\n",
    "    print(f'F1 Score (macro): {f1:.4f}')\n",
    "\n",
    "    # 计算召回率 (宏平均)\n",
    "    recall = recall_score(all_true_labels, all_predictions, average='macro', zero_division=0)\n",
    "    print(f'Recall (macro): {recall:.4f}')\n",
    "\n",
    "    # 计算精确率 (宏平均)\n",
    "    precision = precision_score(all_true_labels, all_predictions, average='macro', zero_division=0)\n",
    "    print(f'Precision (macro): {precision:.4f}')\n",
    "\n",
    "    # 计算每个类别的指标\n",
    "    print('\\n=== Per-Class Metrics ===')\n",
    "    # 获取唯一类别\n",
    "    unique_classes = np.unique(np.concatenate((all_true_labels, all_predictions)))\n",
    "    unique_classes = sorted(unique_classes[unique_classes != 999])  # 排除999标签\n",
    "\n",
    "    class_report = classification_report(all_true_labels, all_predictions, \n",
    "                                        labels=unique_classes,\n",
    "                                        zero_division=0,\n",
    "                                        output_dict=True)\n",
    "\n",
    "    # 打印每个类别的性能指标\n",
    "    for class_label in unique_classes:\n",
    "        print(f'Class {class_label}:')\n",
    "        print(f'  Precision: {class_report[str(class_label)][\"precision\"]:.4f}')\n",
    "        print(f'  Recall: {class_report[str(class_label)][\"recall\"]:.4f}')\n",
    "        print(f'  F1-score: {class_report[str(class_label)][\"f1-score\"]:.4f}')\n",
    "        print(f'  Support: {class_report[str(class_label)][\"support\"]}')\n",
    "\n",
    "    # Calculate clustering time and total execution time\n",
    "    clustering_end = time.time()\n",
    "    clustering_time = clustering_end - preprocessing_end\n",
    "\n",
    "    print(f\"\\n=== Clustering Time ===\")\n",
    "    print(f\"Clustering time: {str(timedelta(seconds=clustering_time))}\")\n",
    "\n",
    "\n",
    "    print(f\"\\n=== Total Time ===\")\n",
    "    total_time = clustering_end - start_time\n",
    "    print(f\"Total execution time: {str(timedelta(seconds=total_time))}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trident_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
