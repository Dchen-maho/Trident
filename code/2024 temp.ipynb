{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "594519b7",
   "metadata": {},
   "source": [
    "补充2类的实验结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62b13ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_process.py\n",
    "# 主程序入口，进行数据加载、预处理、模型训练与测试，并评估分类准确率\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from torch import nn\n",
    "from data_loader import *\n",
    "from sklearn.metrics import *\n",
    "from sklearn.preprocessing import *\n",
    "from collections import Counter\n",
    "from autoencoder import *\n",
    "from evt import *\n",
    "import argparse\n",
    "import time\n",
    "from datetime import timedelta\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "import warnings\n",
    "\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb7b197",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Study\\Github\\Trident\\code\\data_loader.py:76: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data = data.replace([np.inf, -np.inf], np.nan)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution after oversampling:\n",
      "Label\n",
      "11    220202\n",
      "8     195419\n",
      "9     125020\n",
      "10    118008\n",
      "1      19138\n",
      "2      18546\n",
      "5      10097\n",
      "3       7242\n",
      "0       1645\n",
      "4        600\n",
      "6        600\n",
      "7        600\n",
      "Name: count, dtype: int64\n",
      "=== Data Loading Time ===\n",
      "Data loading time: 0:00:05.332618\n"
     ]
    }
   ],
   "source": [
    "# 设置随机种子，保证实验可复现\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# 解析命令行参数，支持自定义数据集名称\n",
    "default_data_name = 'demo'\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--data_name', type = str, default = default_data_name, help = 'data name')\n",
    "# args = parser.parse_args()\n",
    "# data_name = args.data_name\n",
    "data_name = default_data_name\n",
    "\n",
    "# 加载数据集，并进行标准化\n",
    "# data_load: 特征数据, label_load: 标签\n",
    "data_load, label_load = read_dataset2024(data_name)\n",
    "data_load = StandardScaler().fit_transform(data_load)\n",
    "\n",
    "# 统计每个类别的样本数，确定训练/测试划分\n",
    "count_number = Counter(label_load)\n",
    "min_num = np.array(list(count_number.values())).min()  # 最小类别样本数\n",
    "test_per_class = 300  # 每类测试样本数\n",
    "num_per_class = min_num - test_per_class  # 每类训练样本数\n",
    "dim = data_load.shape[1]  # 特征维度\n",
    "b_size = test_per_class   # 批量大小\n",
    "loss_func = nn.MSELoss()  # 损失函数\n",
    "\n",
    "sum_num = len(set(list((label_load))))  # 类别总数\n",
    "train_num = 2  # 训练类别数（只用一个类别做已知，其余为新类）\n",
    "newclass_num = sum_num - train_num  # 新类别数\n",
    "\n",
    "# 打乱数据顺序\n",
    "shun = list(range(data_load.shape[0]))\n",
    "random.shuffle(shun)\n",
    "data_load = data_load[shun]\n",
    "label_load = label_load[shun]\n",
    "\n",
    "# 按数字顺序排列类别索引，而不是随机排列\n",
    "# allIndex = np.arange(train_num + newclass_num)\n",
    "allIndex = np.random.permutation(train_num + newclass_num)\n",
    "\n",
    "\n",
    "# 计算数据加载部分耗费的时间\n",
    "data_loading_end = time.time()\n",
    "data_loading_time = data_loading_end - start_time\n",
    "print(f\"=== Data Loading Time ===\")\n",
    "print(f\"Data loading time: {str(timedelta(seconds=data_loading_time))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb67499a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct labels in 'gtlabel': [11.  7. 10.  1.  3.  9.  6.  2.  4.  0.  8.  5.]\n",
      "\n",
      "=== Data Preprocessing Time ===\n",
      "Preprocessing time: 0:00:00.151524\n"
     ]
    }
   ],
   "source": [
    "# 构建训练集（只包含已知类别）\n",
    "data = np.zeros((num_per_class * (train_num), dim))\n",
    "label = np.zeros(num_per_class * (train_num))\n",
    "for pos in range(train_num):\n",
    "    i = allIndex[pos]\n",
    "    data[pos * num_per_class:(pos + 1) * num_per_class,:] = data_load[label_load==i][0:num_per_class, :]\n",
    "    label[pos * num_per_class:(pos + 1) * num_per_class] = i\n",
    "\n",
    "\n",
    "# 构建流式测试集（包含所有类别，已知类标记为原标签，未知类标记为999）\n",
    "streamdata = np.zeros((test_per_class * (train_num + newclass_num), dim))\n",
    "streamlabel = np.zeros(test_per_class * (train_num + newclass_num))\n",
    "gtlabel = np.zeros(test_per_class * (train_num + newclass_num))\n",
    "for pos in range(train_num + newclass_num):\n",
    "    i = allIndex[pos]\n",
    "    streamdata[pos * test_per_class:(pos + 1) * test_per_class,:] = data_load[label_load==i][-test_per_class:, :]\n",
    "    gtlabel[pos * test_per_class:(pos + 1) * test_per_class] = i\n",
    "    if pos < train_num:\n",
    "        streamlabel[pos * test_per_class:(pos+1) * test_per_class] = i\n",
    "    else:\n",
    "        streamlabel[pos * test_per_class:(pos + 1) * test_per_class] = 999\n",
    "        \n",
    "# 输出 gtlabel 中按出现顺序的不同标签\n",
    "unique_labels = pd.Series(gtlabel).unique()\n",
    "print(\"Distinct labels in 'gtlabel':\", unique_labels)\n",
    "\n",
    "\n",
    "# 根据标签统计，筛选样本数大于50的类别\n",
    "# 返回当前存在的类别列表\n",
    "def make_lab(label):\n",
    "    xianyou = pd.DataFrame(label).value_counts()\n",
    "    curr_lab = []\n",
    "    for j1 in xianyou.keys():\n",
    "        for i in range(train_num):\n",
    "            if (xianyou[j1] > 50) and (j1[0] == allIndex[i]):\n",
    "                curr_lab.append(j1[0])\n",
    "                break\n",
    "        # if xianyou[j1] > 50:\n",
    "        #     curr_lab.append(j1[0])\n",
    "    return curr_lab\n",
    "\n",
    "# 针对每个类别训练自编码器模型，并用SPOT方法确定阈值\n",
    "# 返回模型列表、阈值列表、类别列表\n",
    "def train(data, label, curr_lab):\n",
    "    mod_ls = []\n",
    "    thred_ls = []\n",
    "    class_ls = []\n",
    "    batch = 10\n",
    "    epoch = 10\n",
    "    y_in, y1, y2, y3, y4 = data_load.shape[1], 256, 128, 64, 32\n",
    "    for i in curr_lab:\n",
    "        class_ls.append(i)\n",
    "        model = Autoencoder(y_in, y1, y2, y3, y4)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr = 0.001, weight_decay = 5e-4)\n",
    "        # 训练自编码器\n",
    "        for i2 in range(epoch):\n",
    "            shun = list(range(data[label==i].shape[0]))\n",
    "            random.shuffle(shun)\n",
    "            for i3 in range(int(data[label==i].shape[0] / batch)):\n",
    "                data_input = torch.from_numpy(data[label==i][shun][i3 * batch : (i3+1) * batch]).float()\n",
    "                pred = model(data_input)\n",
    "                loss = loss_func(pred, data_input)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        mod_eva = model.eval()\n",
    "        mod_ls.append(model)\n",
    "        # 计算重构误差，用于阈值确定\n",
    "        mse_ls = []\n",
    "        for i4 in range(int(data[label==i].shape[0] / batch)):\n",
    "            data_input = torch.from_numpy(data[label==i][i4 * batch : (i4+1) * batch]).float()\n",
    "            pred = model(data_input)\n",
    "            for i5 in range(pred.shape[0]):\n",
    "                loss = loss_func(pred[i5], data_input[i5])\n",
    "                mse_ls.append(float(loss.detach().numpy()))\n",
    "        data_input = torch.from_numpy(data[label==i][(int(data[label==i].shape[0] / batch)) * batch:]).float()\n",
    "        pred = model(data_input)\n",
    "        for i5 in range(pred.shape[0]):\n",
    "            loss = loss_func(pred[i5], data_input[i5])\n",
    "            mse_ls.append(float(loss.detach().numpy()))\n",
    "        loss_list_use = np.array(mse_ls)\n",
    "        q = 5e-2 # 风险参数，可调\n",
    "        if len(loss_list_use) == 0:\n",
    "            thred_ls.append(0)\n",
    "            continue\n",
    "        # if len(loss_list_use) < 50 or np.all(loss_list_use == loss_list_use[0]):\n",
    "        #     thred_ls.append(np.max(loss_list_use) + 1e-3)\n",
    "        #     continue\n",
    "        try:\n",
    "            s = SPOT(q)\n",
    "            s.fit(loss_list_use, loss_list_use)\n",
    "            s.initialize()\n",
    "            results = s.run_simp()\n",
    "            # 阈值选取\n",
    "            if results['thresholds'][0] > 0:\n",
    "                thred_ls.append(results['thresholds'][0])\n",
    "            else:\n",
    "                thred_ls.append(np.sort(s.init_data)[int(0.85 * s.init_data.size)])\n",
    "        except Exception as e:\n",
    "            thred_ls.append(np.max(loss_list_use) + 1e-3)\n",
    "    return mod_ls, thred_ls, class_ls\n",
    "\n",
    "# 计算数据预处理部分耗费的时间\n",
    "preprocessing_end = time.time()\n",
    "preprocessing_time = preprocessing_end - data_loading_end\n",
    "\n",
    "print(f\"\\n=== Data Preprocessing Time ===\")\n",
    "print(f\"Preprocessing time: {str(timedelta(seconds=preprocessing_time))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f150f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Initializing ===\n",
      "Initial prediction accuracy (first 2 classes): 0.8900\n",
      "Initial F1 Score: 0.6233\n",
      "Initial Recall: 0.5933\n"
     ]
    }
   ],
   "source": [
    "# 主流程入口\n",
    "if __name__ == '__main__':\n",
    "    print('=== Initializing ===')\n",
    "    # 获取当前类别\n",
    "    curr_num = train_num\n",
    "    curr_lab = allIndex[:curr_num]\n",
    "    # 训练初始模型\n",
    "    mod_ls, thred_ls, class_ls = train(data, label, curr_lab)\n",
    "    \n",
    "    res_ls = []  # 预测结果列表\n",
    "    # 先预测已知的train_num个类别\n",
    "    for i5 in range(test_per_class * train_num):\n",
    "        # 对当前样本用所有模型计算重构误差\n",
    "        data_input = torch.from_numpy(streamdata[i5]).float()\n",
    "        mse_test = []\n",
    "        for model in mod_ls:\n",
    "            mod_eva = model.eval()\n",
    "            pred = model(data_input)\n",
    "            loss = loss_func(pred, data_input)\n",
    "            mse_test.append(float(loss.detach().numpy()))\n",
    "        # 判断是否为新类\n",
    "        cand_res = np.array(mse_test)[np.array(mse_test) < np.array(thred_ls)]\n",
    "        if len(cand_res) == 0:\n",
    "            res_ls.append(999)\n",
    "        else:\n",
    "            min_loss_res = cand_res.min()\n",
    "            res_ls.append(class_ls[mse_test.index(min_loss_res)])\n",
    "    # Calculate accuracy for the first set of predictions\n",
    "    initial_preds = np.array(res_ls)\n",
    "    initial_true = gtlabel[:len(initial_preds)]\n",
    "    initial_accuracy = accuracy_score(initial_true, initial_preds)\n",
    "    print(f\"Initial prediction accuracy (first {train_num} classes): {initial_accuracy:.4f}\")\n",
    "\n",
    "    # Calculate other metrics for the initial prediction\n",
    "    initial_f1 = f1_score(initial_true, initial_preds, average='macro', zero_division=0)\n",
    "    initial_recall = recall_score(initial_true, initial_preds, average='macro', zero_division=0)\n",
    "    print(f\"Initial F1 Score: {initial_f1:.4f}\")\n",
    "    print(f\"Initial Recall: {initial_recall:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trident_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
